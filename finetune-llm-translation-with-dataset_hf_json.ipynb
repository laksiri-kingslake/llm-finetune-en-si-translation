{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read data from a JSON file\n",
    "import json\n",
    "\n",
    "file = json.load(open(\"dataset_hf.json\", \"r\"))\n",
    "print(file[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a8b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install dependencies\n",
    "%pip install unsloth trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac94e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Check for the GPU\n",
    "# For GPU check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90853e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Get the model\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
    "\n",
    "max_seq_length = 2048  # Choose sequence length\n",
    "dtype = None  # Auto detection\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e60e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Format the dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    return f\"### Input: {example['english']}\\n### Output: {example['sinhala']}<|endoftext|>\"\n",
    "\n",
    "formatted_data = [format_prompt(item) for item in file]\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57860537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Add LoRA adapter\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # LoRA rank - higher = more capacity, more memory\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # Rank stabilized LoRA\n",
    "    loftq_config=None, # LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4957c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Set training arguments\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments optimized for Unsloth\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=25,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\", # Disable Weights & Biases logging\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Train the model\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Test the fine-tuned model\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Test prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following English sentence to Sinhala:\\n What is your name?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = inputs.ne(tokenizer.pad_token_id).long().to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    attention_mask=attention_mask, # Pass attention mask\n",
    "    max_new_tokens=256,\n",
    "    use_cache=True,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "# Decode Unicode escape sequences\n",
    "print(response.encode('utf-8').decode('unicode_escape').encode('utf-8').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a99bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Save the model\n",
    "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bd71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Download the model\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
    "if gguf_files:\n",
    "    gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
    "    print(f\"Downloading: {gguf_file}\")\n",
    "    files.download(gguf_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
